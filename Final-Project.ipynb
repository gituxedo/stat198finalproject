{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.tsa.stattools as st\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.vector_ar import var_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stock and Watson (2002) argue that factor models are either able to cope with breaks in the factor loadings in a fraction of the series, or can account for moderate parameter drift in all of the series. However, in empirical applications parameters may change dramatically due to important economic events. There may also be more gradual but nevertheless fundamental changes in economic structures that may have led to significant changes in the comovements of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, time-varying factor loadings lead not only to inconsistent estimates of the loadings but also to a larger dimension of the factor space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA: Macroeconomic Database FRED-MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macroeconomic database of monthly U.S. indicators \n",
    "# such as output and income, the labor market and prices from 1959 to 2020\n",
    "df = pd.read_csv(r\"current.csv\")\n",
    "# according to the FRED-MD paper, these 7 factors have the greatest explanatory power, so we will use them from now on\n",
    "fred7 = df[['USGOOD', 'PAYEMS', 'MANEMP', 'IPMANSICS', 'DMANEMP', 'INDPRO', 'CUMFNS']].copy()\n",
    "# drop any column that has a nan value, as well as the first \"transformation\" row\n",
    "fred7 = fred7.dropna()[1:]\n",
    "# it turns out coronavirus is causing a huge number of outliers in our data, so we will drop the months after march 2020\n",
    "fred7 = fred7[:-8]\n",
    "# grab the dates for plotting purposes\n",
    "dates = df['sasdate'][1:-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform it into an numpy ndarray\n",
    "fred7 = fred7.to_numpy().T\n",
    "fred7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust for the non-stationarity in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference the non-stationary series to stationarity\n",
    "# check for non-stationary and take differences of non-stationary series\n",
    "def non_stationary(timeseries):\n",
    "    # Perform augmented Dickey-Fuller test and return whether the test statistic is greater than the critical value,\n",
    "    # which means the time series is non-stationary. We pick an alpha = 0.10\n",
    "    dftest = st.adfuller(timeseries, autolag='AIC')\n",
    "    test_statistic = dftest[0]\n",
    "    crit_value = dftest[4]['10%']\n",
    "    return test_statistic > crit_value\n",
    "\n",
    "stationary_fred7 = fred7.copy()\n",
    "# print(len(stationary_fred7))\n",
    "# print(len(stationary_fred7[0]))\n",
    "for i in range(len(stationary_fred7)):\n",
    "    if non_stationary(stationary_fred7[i]):\n",
    "        diffs = []\n",
    "        for j in range(1, len(stationary_fred7[i])):\n",
    "            diffs.append(stationary_fred7[i][j] - stationary_fred7[i][j-1])\n",
    "        diffs.insert(0, np.mean(diffs))\n",
    "        stationary_fred7[i] = diffs\n",
    "\n",
    "# print(stationary_fred7.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center(series):\n",
    "    zero_mean = series - np.mean(series)\n",
    "    unit_variance = zero_mean / (np.std(series))\n",
    "    return unit_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize all columns to have zero mean and unit variance\n",
    "whitened_fred7 = np.empty(shape=(0,0))\n",
    "for col in stationary_fred7:\n",
    "    # don't apply this to malformed columns, i.e. columns with nan or non-floats\n",
    "    if not isinstance(col[0], str):\n",
    "        whitened_fred7 = np.append(whitened_fred7, center(col))\n",
    "whitened_fred7 = whitened_fred7.reshape(stationary_fred7.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot([np.log(i) for i in fred7[1]])\n",
    "# plt.plot([np.log(i) for i in fred7[2]])\n",
    "# plt.plot([np.log(i) for i in fred7[3]])\n",
    "plt.plot(whitened_fred7[0])\n",
    "plt.plot(whitened_fred7[1])\n",
    "plt.plot(whitened_fred7[2])\n",
    "plt.plot(whitened_fred7[3])\n",
    "plt.plot(whitened_fred7[4])\n",
    "plt.plot(whitened_fred7[5])\n",
    "plt.plot(whitened_fred7[6])\n",
    "# plt.legend(whitened_fred7[0], ['USGOOD', 'PAYEMS', 'MANEMP', 'IPMANSICS', 'DMANEMP', 'INDPRO', 'CUMFNS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(index):\n",
    "    fig, axs = plt.subplots(3)\n",
    "    fig.figsize = [6.4, 9.6]\n",
    "    axs[0].plot(fred7[index])\n",
    "    axs[1].plot(stationary_fred7[index])\n",
    "    axs[2].plot(whitened_fred7[index])\n",
    "plotter(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_cols = ['feature'+str(i) for i in range(whitened_fred7.shape[0])]\n",
    "normalized_fred7 = pd.DataFrame(whitened_fred7.T,columns=feat_cols)\n",
    "normalized_fred7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the normalized aggregate data we will use to test for time-varying loadings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all principle components of the aggregate macro data\n",
    "N = 6\n",
    "pca_fred = PCA(n_components = N)\n",
    "principalComponents_fred7 = pca_fred.fit_transform(normalized_fred7)\n",
    "principalComponents_fred7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_fred_df = pd.DataFrame(data = principalComponents_fred7\n",
    "             , columns = ['PC'+str(i) for i in range(1, N+1)])\n",
    "principal_fred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_explained_variance = pca_fred.explained_variance_ratio_\n",
    "fred_explained_variance_df = pd.DataFrame(fred_explained_variance, columns = ['Explained Variance'])\n",
    "fred_explained_variance_df.rename(index = lambda x: 'PC'+str(x+1), inplace=True)\n",
    "fred_explained_variance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(fred_explained_variance))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for time-varying loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the null hypothesis that the time loadings are time-invariant by testing the autocorrelation in the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine lag order from the average of all features\n",
    "N = len(normalized_fred7.columns)\n",
    "T = len(normalized_fred7.index)\n",
    "optimal_lag = np.array([])\n",
    "for j in range(0, N):\n",
    "    k = 0\n",
    "    highestCorr = 0\n",
    "    for i in range(1, 10):\n",
    "        cor = pd.Series.autocorr(normalized_fred7.iloc[:, j].astype(float), lag = i)\n",
    "        if(cor > highestCorr):\n",
    "            highestCorr = cor\n",
    "            k = i\n",
    "    optimal_lag = np.append(optimal_lag, k)\n",
    "optimal_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = int(np.round(np.mean(optimal_lag)))\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the rejection rate of the null hypothesis that the autocorrelations of the residual time series are not different from zero (also called the Ljungâ€“Box test), thus testing for serial correlation between the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_fred_10 = normalized_fred7\n",
    "model = VAR(test_fred_10.astype(float))\n",
    "results_10 = model.fit(p, method = \"ols\")\n",
    "results_10.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_test_10 = var_model.VARResults.test_whiteness(results_10, nlags=10, signif=0.05, adjusted=False)\n",
    "resid_test_10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resid_test_10.pvalue <= 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $p \\leq 0.05$, we reject the null hypothesis that the residual autocorrelations are 0. In other words, since the autocorrelations are significantly different from 0, the data is serially correlated, which shows seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejection_10 = np.array([])\n",
    "for j in range(1, int(N/n)):\n",
    "    test_fred = normalized_fred7\n",
    "    model = VAR(test_fred.astype(float))\n",
    "    results = model.fit(p, method = \"ols\")\n",
    "    resid_test = var_model.VARResults.test_whiteness(results, nlags=250, signif=0.05, adjusted=False)\n",
    "    rejection_10 = np.append(rejection_10, resid_test.pvalue)\n",
    "rejection_10\n",
    "\n",
    "rejection_CDF = pd.Series([var_model.VARResults.test_whiteness(results, nlags=i, signif=0.05, adjusted=False).pvalue for i in range(200, 300)])\n",
    "rejection_PDF = rejection_CDF - rejection_CDF.shift(2)\n",
    "# var_model.VARResults.test_whiteness?\n",
    "# plt.plot(range(200, 300), rejection_CDF)\n",
    "plt.plot(range(200, 300), rejection_PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejection_rate_10 = rejection_10.sum()/len(rejection_10)\n",
    "print(str(rejection_rate_10*100)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejection_rate = np.array([])\n",
    "for i in np.array([10, 20, 50]):\n",
    "    n = i # number of variable-specific tests\n",
    "    j_index = 0\n",
    "    rejection_i = np.array([])\n",
    "    for j in range(1, int(N/i)):\n",
    "        test_fred = normalized_fred7.iloc[:, np.arange(j_index, j_index+n)]\n",
    "        model = VAR(test_fred.astype(float))\n",
    "        results = model.fit(p, method = \"ols\")\n",
    "        resid_test = var_model.VARResults.test_whiteness(results, nlags=10, signif=0.05, adjusted=False)\n",
    "        rejection_i = np.append(rejection_i, resid_test.pvalue <= 0.05)\n",
    "        j_index += n\n",
    "    rejection_rate_i = rejection_i.sum()/len(rejection_i)\n",
    "    rejection_rate = np.append(rejection_rate, str(rejection_rate_10*100)+'%')\n",
    "rejection_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejection_rate_df = pd.DataFrame(rejection_rate, columns = ['Reject'])\n",
    "rejection_rate_df = rejection_rate_df.rename(index = lambda x: 'N = '+ str(np.array([10, 20, 50]).item(x)))\n",
    "rejection_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
